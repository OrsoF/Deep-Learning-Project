{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data processing","metadata":{}},{"cell_type":"code","source":"!pip install imutils","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:40:05.542850Z","iopub.execute_input":"2022-02-13T10:40:05.543622Z","iopub.status.idle":"2022-02-13T10:40:14.418207Z","shell.execute_reply.started":"2022-02-13T10:40:05.543518Z","shell.execute_reply":"2022-02-13T10:40:14.417329Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import zipfile\nimport os\nimport shutil\nimport pandas as pd\nimport ast\nimport random\nfrom imutils import paths\nimport cv2\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.utils import to_categorical\nimport keras\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\nimport pickle\n\n# if 'tensorflow-great-barrier-reef' not in os.listdir(os.getcwd()):\n#     !kaggle competitions download -c tensorflow-great-barrier-reef\n#     with zipfile.ZipFile('tensorflow-great-barrier-reef.zip', 'r') as zip_ref:\n#         zip_ref.extractall('tensorflow-great-barrier-reef')\n#     os.remove('tensorflow-great-barrier-reef.zip')\n    \ndf = pd.read_csv('../input/tensorflow-great-barrier-reef/train.csv')\ndf['annotations'] = df['annotations'].apply(ast.literal_eval)\ndf['path'] = df['image_id'].apply(lambda elem : os.path.join('../input/tensorflow-great-barrier-reef/train_images', os.path.join('video_'+str(elem[0]), elem[2:]+'.jpg')))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:43:10.012796Z","iopub.execute_input":"2022-02-13T10:43:10.013467Z","iopub.status.idle":"2022-02-13T10:43:10.460300Z","shell.execute_reply.started":"2022-02-13T10:43:10.013424Z","shell.execute_reply":"2022-02-13T10:43:10.459513Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"plt.plot(df['annotations'].apply(len))\nplt.ylabel('Number of instance')\nplt.xlabel('Frame')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:40:20.342214Z","iopub.execute_input":"2022-02-13T10:40:20.342481Z","iopub.status.idle":"2022-02-13T10:40:20.553261Z","shell.execute_reply.started":"2022-02-13T10:40:20.342445Z","shell.execute_reply":"2022-02-13T10:40:20.552415Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print('Number of frame with one instance :')\nprint(sum(elem == 1 for elem in df['annotations'].apply(len)))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:40:20.555455Z","iopub.execute_input":"2022-02-13T10:40:20.555743Z","iopub.status.idle":"2022-02-13T10:40:20.578271Z","shell.execute_reply.started":"2022-02-13T10:40:20.555689Z","shell.execute_reply":"2022-02-13T10:40:20.577387Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print('Histogram of number of COTS instances :')\n_ = []\nfor elem in df['annotations'].apply(len):\n    if elem >0:\n        _.append(elem)\n        \nplt.hist(_, bins = 18)\nplt.ylabel('Number of frame')\nplt.xlabel('Number of instance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:40:20.580046Z","iopub.execute_input":"2022-02-13T10:40:20.580306Z","iopub.status.idle":"2022-02-13T10:40:20.816942Z","shell.execute_reply.started":"2022-02-13T10:40:20.580270Z","shell.execute_reply":"2022-02-13T10:40:20.816204Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"TRAIN_RATIO = 0.8\n\ndf_test = df.iloc[int(TRAIN_RATIO*df.shape[0]):]\ndf = df.iloc[:int(TRAIN_RATIO*df.shape[0])]\n\nimages_with_cots = df[(df['annotations'].apply(len) > 0)].index\nimages_without_cots = df[(df['annotations'].apply(len) == 0)].index\n\nMAX_PROPOSALS = 5000\nMAX_PROPOSALS_INFER = 5000\n\nMAX_POSITIVE = 30\nMAX_NEGATIVE = 3\n\ntry:\n    os.mkdir('data')\nexcept:\n    pass\n\nPOSITIVE_PATH = os.path.join(os.getcwd(), os.path.join('data','positive'))\nNEGATIVE_PATH = os.path.join(os.getcwd(), os.path.join('data','negative'))\ntry:\n    os.mkdir(POSITIVE_PATH)\n    os.mkdir(NEGATIVE_PATH)\nexcept:\n    pass\n\nh, w = plt.imread(df.iloc[0]['path']).shape[:2]\n\nINPUT_DIMS = (224, 224)\n\nMODEL_PATH = \"cots_detector.h5\"\nENCODER_PATH = \"label_encoder.pickle\"\n\n# define the minimum probability required for a positive prediction\n# (used to filter out false-positive predictions)\nMIN_PROBA = 0.99\n\ndef compute_iou(boxA, boxB):\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    return iou","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:43:30.688345Z","iopub.execute_input":"2022-02-13T10:43:30.688738Z","iopub.status.idle":"2022-02-13T10:43:30.837474Z","shell.execute_reply.started":"2022-02-13T10:43:30.688671Z","shell.execute_reply":"2022-02-13T10:43:30.836412Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if 'rcnnpreprocess' not in os.listdir('../input'):\n    imagePaths = df['path']\n\n    totalPositive = 0\n    totalNegative = 0\n\n    for (i, imagePath) in enumerate(imagePaths):\n        idx = imagePaths.index[i]\n        if len(df.iloc[idx]['annotations']) == 0:\n            continue\n        print(\"[INFO] processing image {}/{}...\".format(i + 1, len(imagePaths)))\n\n        gtBoxes = [(elem['x'], elem['y'], elem['x']+elem['width'], elem['y']+elem['height']) for elem in df.iloc[i]['annotations']]\n\n        image = cv2.imread(imagePath)\n        ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n        ss.setBaseImage(image)\n        ss.switchToSelectiveSearchFast()\n        rects = ss.process()\n        proposedRects= []\n        for (x, y, w, h) in rects:\n            proposedRects.append((x, y, x + w, y + h))\n\n        positiveROIs = 0\n        negativeROIs = 0\n        for proposedRect in proposedRects[:MAX_PROPOSALS]:\n            (propStartX, propStartY, propEndX, propEndY) = proposedRect\n            for gtBox in gtBoxes:\n                iou = compute_iou(gtBox, proposedRect)\n                (gtStartX, gtStartY, gtEndX, gtEndY) = gtBox\n                roi = None\n                outputPath = None\n\n                if iou > 0.7 and positiveROIs <= MAX_POSITIVE:\n                    roi = image[propStartY:propEndY, propStartX:propEndX]\n                    filename = \"{}.png\".format(totalPositive)\n                    outputPath = os.path.sep.join([POSITIVE_PATH, filename])\n                    positiveROIs += 1\n                    totalPositive += 1\n\n                fullOverlap = propStartX >= gtStartX\n                fullOverlap = fullOverlap and propStartY >= gtStartY\n                fullOverlap = fullOverlap and propEndX <= gtEndX\n                fullOverlap = fullOverlap and propEndY <= gtEndY\n\n                if not fullOverlap and iou < 0.05 and \\\n                    negativeROIs <= MAX_NEGATIVE:\n\n                    roi = image[propStartY:propEndY, propStartX:propEndX]\n                    filename = \"{}.png\".format(totalNegative)\n                    outputPath = os.path.sep.join([NEGATIVE_PATH, filename])\n\n                    negativeROIs += 1\n                    totalNegative += 1\n\n                if roi is not None and outputPath is not None:\n                    roi = cv2.resize(roi, INPUT_DIMS,\n                        interpolation=cv2.INTER_CUBIC)\n                    cv2.imwrite(outputPath, roi)\n\n\n    # Balance data\n    positive_images = os.listdir('positive')\n    n_ = len(os.listdir('negative'))\n    i = 0\n    while len(os.listdir('positive')) < n_:\n        elem = random.choice(positive_images)\n        shutil.copyfile(os.path.join('positive', elem), os.path.join('positive', str(i)+'_'+elem))\n        i += 1\nelse:\n    pass","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-13T10:40:20.926759Z","iopub.execute_input":"2022-02-13T10:40:20.927029Z","iopub.status.idle":"2022-02-13T10:40:20.945609Z","shell.execute_reply.started":"2022-02-13T10:40:20.926992Z","shell.execute_reply":"2022-02-13T10:40:20.944633Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"INIT_LR = 1e-4\nEPOCHS = 50\nBS = 8\n\nimg_height, img_width = INPUT_DIMS\ndata_dir = '../input/rcnnpreprocess/data'\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    directory = data_dir, label_mode='categorical',\n    class_names=['positive', 'negative'], color_mode='rgb', batch_size=BS, image_size=(224,\n    224), shuffle=True, subset=\"training\", seed = 1, validation_split=0.2\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    directory = data_dir, label_mode='categorical',\n    class_names=['positive', 'negative'], color_mode='rgb', batch_size=BS, image_size=(224,\n    224), shuffle=True, subset=\"validation\", seed = 1, validation_split=0.2\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:43:38.231337Z","iopub.execute_input":"2022-02-13T10:43:38.231655Z","iopub.status.idle":"2022-02-13T10:43:55.896084Z","shell.execute_reply.started":"2022-02-13T10:43:38.231617Z","shell.execute_reply":"2022-02-13T10:43:55.893791Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# class_names = train_ds.class_names\n# print(class_names)\n\n# AUTOTUNE = tf.data.AUTOTUNE\n# train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n# val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n\n# plt.figure(figsize=(10, 10))\n# for images, labels in train_ds.take(1):\n#     for i in range(9):\n#         ax = plt.subplot(3, 3, i + 1)\n#         plt.imshow(images[i].numpy().astype(\"uint8\"))\n#         plt.title(class_names[np.argmax(labels[i])])\n#         plt.axis(\"off\")\n    \n# for image_batch, labels_batch in train_ds:\n#     print(image_batch.shape)\n#     print(labels_batch.shape)\n#     break","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:40:38.525060Z","iopub.execute_input":"2022-02-13T10:40:38.525858Z","iopub.status.idle":"2022-02-13T10:41:50.954764Z","shell.execute_reply.started":"2022-02-13T10:40:38.525813Z","shell.execute_reply":"2022-02-13T10:41:50.954082Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.05),\n    layers.RandomZoom((-0.05, 0.05)),\n    layers.RandomTranslation((-0.1, 0.1), (-0.1, 0.1)),\n    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n    layers.Conv2D(16, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(32, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(64, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dense(2, activation='sigmoid')\n])\n\nprint(\"[INFO] compiling model...\")\nopt = Adam(learning_rate=INIT_LR)\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:43:55.898050Z","iopub.execute_input":"2022-02-13T10:43:55.898330Z","iopub.status.idle":"2022-02-13T10:43:55.948176Z","shell.execute_reply.started":"2022-02-13T10:43:55.898289Z","shell.execute_reply":"2022-02-13T10:43:55.947404Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=EPOCHS\n)\n\nprint(\"[INFO] saving mask detector model...\")\nmodel.save(MODEL_PATH, save_format=\"h5\")","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:46:32.318175Z","iopub.execute_input":"2022-02-13T10:46:32.318791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the training loss and accuracy\nN = EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.savefig('plot.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def predict(img):\n    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n    ss.setBaseImage(image)\n    ss.switchToSelectiveSearchFast()\n    rects = ss.process()\n    \n    proposals = []\n    boxes = []\n    \n    for (x, y, w, h) in rects[:MAX_PROPOSALS_INFER]:\n        roi = image[y:y + h, x:x + w]\n        roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n        roi = cv2.resize(roi, INPUT_DIMS, interpolation=cv2.INTER_CUBIC)\n        roi = img_to_array(roi)\n        roi = preprocess_input(roi)\n        proposals.append(roi)\n        boxes.append((x, y, w, h))\n        \n    proposals = np.array(proposals, dtype=\"float32\")\n    boxes = np.array(boxes, dtype=\"int32\")\n    \n    annot = []\n    \n    proba = model.predict(proposals)\n    for i, elem in enumerate(proba):\n        if elem[1] > 0.7:\n            annot.append('{:.2f} {} {} {} {}'.format(elem[1], int(boxes[i][0]), int(boxes[i][1]), int(boxes[i][2]), int(boxes[i][3])))\n    return ' '.join(annot)\n\n\n\nimport greatbarrierreef\nenv = greatbarrierreef.make_env()\niter_test = env.iter_test() \n\nfor (image_np, sample_prediction_df) in iter_test:\n    pred = predict(image_np)\n    sample_prediction_df['annotations'] = pred\n    env.predict(sample_prediction_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}